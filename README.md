# Oracle CDC with OpenLogReplicator, Debezium, and Kafka

A production-ready Change Data Capture (CDC) pipeline that streams Oracle database changes to Apache Kafka using OpenLogReplicator and Debezium.

![Architecture Diagram](img_1.png)

## Table of Contents

- [Prerequisites](#prerequisites)
- [Architecture Overview](#architecture-overview)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
  - [Directory Setup](#1-directory-setup)
  - [Oracle and OpenLogReplicator Setup](#2-oracle-and-openlogreplicator-setup)
  - [Kafka Ecosystem Setup](#3-kafka-ecosystem-setup)
- [Connector Configuration](#connector-configuration)
- [Verification](#verification)
- [Troubleshooting](#troubleshooting)

## Prerequisites

- Docker Engine 20.10+
- Docker Compose v2.0+
- Minimum 16GB RAM (8GB for Oracle, 4GB for Kafka)
- 20GB+ available disk space
- Access to Oracle Container Registry (for Oracle database image)

## Architecture Overview

| Component | Description |
|-----------|-------------|
| Oracle 19c | Source database with ARCHIVELOG enabled |
| OpenLogReplicator | High-performance Oracle redo log reader |
| Debezium | CDC connector framework for Kafka Connect |
| Apache Kafka | Distributed event streaming platform (3-node cluster) |
| Schema Registry | Avro schema management for Kafka |
| Kafka UI | Web interface for monitoring |
| PostgreSQL | Optional sink database |

## Quick Start

```bash
# 1. Create Docker network
docker network create debezium-net

# 2. Create required directories
mkdir -p olr-checkpoint olr-log scripts-db scripts
sudo chown -R 1000:1000 olr-checkpoint olr-log scripts scripts-db

# 3. Start Oracle and OpenLogReplicator
docker compose -f docker-compose-olr.yml up -d

# 4. Wait for Oracle to be healthy, then start Kafka ecosystem
docker compose -f docker-compose-main.yml up -d

# 5. Access Kafka UI
open http://localhost:18000
```

---

## Configuration

### 1. Directory Setup

Create the required directories and set appropriate permissions:

```bash
# Create directories
mkdir -p olr-checkpoint olr-log scripts-db scripts

# Set ownership for OpenLogReplicator access
sudo chown -R 1000:1000 olr-checkpoint olr-log scripts scripts-db
```

### 2. Oracle and OpenLogReplicator Setup

#### 2.1 Create Docker Network

```bash
docker network create debezium-net
```

#### 2.2 Permission Script

Create `scripts-db/permission.sh`:

```bash
#!/bin/bash
# Set permissions for OpenLogReplicator to access Oracle redo logs and datafiles

chmod -R 755 /opt/oracle/oradata/ORCLCDB 2>/dev/null || true
chmod -R 755 /opt/oracle/fast_recovery_area 2>/dev/null || true
```

#### 2.3 Oracle Setup SQL

Create `scripts-db/set-up-olr.sql`:

```sql
SET ECHO ON;
SET SERVEROUTPUT ON;
WHENEVER SQLERROR EXIT FAILURE;

-- 1. Enable ARCHIVELOG mode (skip when already enable from docker-compose)
DECLARE
    v_log_mode VARCHAR2(10);
BEGIN
    SELECT log_mode INTO v_log_mode FROM v$database;
    IF v_log_mode != 'ARCHIVELOG' THEN
        EXECUTE IMMEDIATE 'SHUTDOWN IMMEDIATE';
        EXECUTE IMMEDIATE 'STARTUP MOUNT';
        EXECUTE IMMEDIATE 'ALTER DATABASE ARCHIVELOG';
        EXECUTE IMMEDIATE 'ALTER DATABASE OPEN';
        DBMS_OUTPUT.PUT_LINE('ARCHIVELOG mode enabled.');
    ELSE
        DBMS_OUTPUT.PUT_LINE('ARCHIVELOG mode already enabled.');
    END IF;
END;
/

-- Set recovery area size
ALTER SYSTEM SET db_recovery_file_dest_size = 10G SCOPE=BOTH;

-- Enable minimal supplemental logging (required for both LogMiner and OLR)
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;

-- 2. Create LogMiner tablespace in CDB (idempotent)
BEGIN
    EXECUTE IMMEDIATE 'CREATE TABLESPACE LOGMINER_TBS DATAFILE ''/opt/oracle/oradata/ORCLCDB/logminer_tbs.dbf'' SIZE 100M REUSE AUTOEXTEND ON NEXT 100M MAXSIZE UNLIMITED';
    DBMS_OUTPUT.PUT_LINE('LOGMINER_TBS tablespace created in CDB.');
EXCEPTION
    WHEN OTHERS THEN
        IF SQLCODE = -1543 THEN
            DBMS_OUTPUT.PUT_LINE('LOGMINER_TBS tablespace already exists in CDB.');
        ELSE
            RAISE;
        END IF;
END;
/

-- Switch to PDB and create tablespace there
ALTER SESSION SET CONTAINER = ORCLPDB1;

BEGIN
    EXECUTE IMMEDIATE 'CREATE TABLESPACE LOGMINER_TBS DATAFILE ''/opt/oracle/oradata/ORCLCDB/ORCLPDB1/logminer_tbs.dbf'' SIZE 100M REUSE AUTOEXTEND ON NEXT 100M MAXSIZE UNLIMITED';
    DBMS_OUTPUT.PUT_LINE('LOGMINER_TBS tablespace created in PDB.');
EXCEPTION
    WHEN OTHERS THEN
        IF SQLCODE = -1543 THEN
            DBMS_OUTPUT.PUT_LINE('LOGMINER_TBS tablespace already exists in PDB.');
        ELSE
            RAISE;
        END IF;
END;
/

-- Back to CDB
ALTER SESSION SET CONTAINER = CDB$ROOT;

-- 3. Create common user c##dbzuser (idempotent)
DECLARE
    v_count NUMBER;
BEGIN
    SELECT COUNT(*) INTO v_count FROM dba_users WHERE username = 'C##DBZUSER';
    IF v_count = 0 THEN
        EXECUTE IMMEDIATE q'[
            CREATE USER c##dbzuser IDENTIFIED BY dbz
            DEFAULT TABLESPACE LOGMINER_TBS
            QUOTA UNLIMITED ON LOGMINER_TBS
            CONTAINER=ALL
        ]';
        DBMS_OUTPUT.PUT_LINE('User c##dbzuser created.');
    ELSE
        DBMS_OUTPUT.PUT_LINE('User c##dbzuser already exists.');
    END IF;
END;
/

-- 4. Grant permissions for c##dbzuser (CDB level)
GRANT CREATE SESSION TO c##dbzuser CONTAINER=ALL;
GRANT SET CONTAINER TO c##dbzuser CONTAINER=ALL;
GRANT CONNECT TO c##dbzuser;
GRANT RESOURCE TO c##dbzuser;

-- Database dictionary access
GRANT SELECT ON V_$DATABASE TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ON V_$LOG TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ON V_$LOGFILE TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ON V_$ARCHIVED_LOG TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ON V_$PARAMETER TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ON V_$TRANSACTION TO c##dbzuser CONTAINER=ALL;

-- LogMiner specific grants
GRANT SELECT ANY DICTIONARY TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ANY TRANSACTION TO c##dbzuser CONTAINER=ALL;
GRANT SELECT_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;

-- Table access grants
GRANT SELECT ANY TABLE TO c##dbzuser CONTAINER=ALL;
GRANT FLASHBACK ANY TABLE TO c##dbzuser CONTAINER=ALL;
GRANT LOCK ANY TABLE TO c##dbzuser CONTAINER=ALL;

-- Dictionary tables access
GRANT SELECT ON DBA_OBJECTS TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ON DBA_TABLES TO c##dbzuser CONTAINER=ALL;
GRANT SELECT ON DBA_TAB_COLUMNS TO c##dbzuser CONTAINER=ALL;

-- LogMiner dictionary grants
GRANT EXECUTE_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;
GRANT CREATE TABLE TO c##dbzuser CONTAINER=ALL;
GRANT CREATE SEQUENCE TO c##dbzuser CONTAINER=ALL;

-- 5. PDB-specific grants for both LogMiner and OpenLogReplicator
ALTER SESSION SET CONTAINER = ORCLPDB1;

-- OpenLogReplicator specific SYS table grants
GRANT SELECT, FLASHBACK ON SYS.CCOL$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.CDEF$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.COL$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.DEFERRED_STG$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.ECOL$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.LOB$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.LOBCOMPPART$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.LOBFRAG$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.OBJ$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.TAB$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.TABCOMPART$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.TABPART$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.TABSUBPART$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.TS$ TO c##dbzuser;
GRANT SELECT, FLASHBACK ON SYS.USER$ TO c##dbzuser;

-- XDB table grants for OpenLogReplicator
GRANT SELECT, FLASHBACK ON XDB.XDB$TTSET TO c##dbzuser;

-- Grant on all XDB internal tables
BEGIN
    FOR t IN (
        SELECT owner, table_name
        FROM dba_tables
        WHERE owner = 'XDB'
        AND (table_name LIKE 'X$NM%'
             OR table_name LIKE 'X$PT%'
             OR table_name LIKE 'X$QN%'
             OR table_name LIKE 'X$%')
    ) LOOP
        BEGIN
            EXECUTE IMMEDIATE 'GRANT SELECT, FLASHBACK ON ' || t.owner || '."' || t.table_name || '" TO c##dbzuser';
            DBMS_OUTPUT.PUT_LINE('Granted on: ' || t.owner || '.' || t.table_name);
        EXCEPTION
            WHEN OTHERS THEN
                DBMS_OUTPUT.PUT_LINE('Failed to grant on: ' || t.owner || '.' || t.table_name || ' - ' || SQLERRM);
        END;
    END LOOP;
END;
/

-- Additional LogMiner grants
GRANT EXECUTE ON DBMS_LOGMNR TO c##dbzuser;
GRANT EXECUTE ON DBMS_LOGMNR_D TO c##dbzuser;

-- 6. Create application schema and table
DECLARE
    v_count NUMBER;
BEGIN
    SELECT COUNT(*) INTO v_count FROM dba_users WHERE username = 'OLR_DB';
    IF v_count = 0 THEN
        EXECUTE IMMEDIATE q'[
            CREATE USER olr_db IDENTIFIED BY olr_password
            DEFAULT TABLESPACE USERS
            QUOTA UNLIMITED ON USERS
            CONTAINER=CURRENT
        ]';
        DBMS_OUTPUT.PUT_LINE('User olr_db created.');
    ELSE
        DBMS_OUTPUT.PUT_LINE('User olr_db already exists.');
    END IF;
END;
/

GRANT CREATE SESSION, CREATE TABLE, CREATE SEQUENCE TO olr_db;
GRANT CONNECT, RESOURCE TO olr_db;

-- 7. Create table and supplemental logging (as olr_db)
CONNECT olr_db/olr_password@ORCLPDB1;

BEGIN
    EXECUTE IMMEDIATE q'[
        CREATE TABLE product (
            id NUMBER(10) GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
            name VARCHAR2(100) NOT NULL,
            description VARCHAR2(500),
            price NUMBER(10,2) NOT NULL,
            stock NUMBER(8) DEFAULT 0,
            created_date DATE DEFAULT SYSDATE,
            updated_date DATE DEFAULT SYSDATE
        )
    ]';
    DBMS_OUTPUT.PUT_LINE('Table product created.');
EXCEPTION
    WHEN OTHERS THEN
        IF SQLCODE = -955 THEN
            DBMS_OUTPUT.PUT_LINE('Table product already exists.');
        ELSE
            RAISE;
        END IF;
END;
/

-- Enable supplemental logging for the table
ALTER TABLE product ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;

-- 8. Grant table access to c##dbzuser
CONNECT / AS SYSDBA;
ALTER SESSION SET CONTAINER = ORCLPDB1;

GRANT SELECT, FLASHBACK ON olr_db.product TO c##dbzuser;

-- 9. Final verification
CONNECT c##dbzuser/dbz@ORCLPDB1;

PROMPT === Verification: Table structure ===
DESCRIBE olr_db.product;

PROMPT === Verification: Row count (should be 0) ===
SELECT COUNT(*) AS row_count FROM olr_db.product;

-- Back to SYS
CONNECT / AS SYSDBA;
ALTER SESSION SET CONTAINER = CDB$ROOT;

PROMPT =====================================================
PROMPT Setup completed successfully!
PROMPT =====================================================
```

#### 2.4 OpenLogReplicator Configuration

Create `scripts/OpenLogReplicator.json`:

```json
{
  "version": "1.8.7",
  "log-level": 4,
  "source": [
    {
      "alias": "SOURCE",
      "name": "ORACLE",
      "reader": {
        "type": "online",
        "user": "c##dbzuser",
        "password": "dbz",
        "server": "//cdc-oracle-olr:1521/ORCLPDB1"
      },
      "format": {
        "type": "json",
        "column": 2,
        "db": 3,
        "interval-dts": 9,
        "interval-ytm": 4,
        "message": 2,
        "rid": 1,
        "schema": 7,
        "timestamp-all": 1,
        "scn-type": 1,
        "unknown-type": 1,
        "xid": 1
      },
      "memory": {
        "min-mb": 64,
        "max-mb": 1024
      },
      "filter": {
        "table": [
          {
            "owner": "OLR_DB",
            "table": "PRODUCT"
          }
        ]
      }
    }
  ],
  "target": [
    {
      "alias": "DEBEZIUM",
      "source": "SOURCE",
      "writer": {
        "type": "network",
        "uri": "0.0.0.0:9000"
      }
    }
  ]
}
```

**Configuration Reference:**

| Field | Description |
|-------|-------------|
| `source.alias` | Identifier linked to target configuration |
| `source.name` | Name used in Debezium connector configuration |
| `reader.server` | Format: `//<container>:<port>/<PDB>` |
| `filter.table` | Tables to capture (schema.table) |
| `target.writer.uri` | Network endpoint for Debezium connection |

#### 2.5 Docker Compose - Oracle and OpenLogReplicator

Create `docker-compose-olr.yml`:

```yaml
services:
  cdc-oracle-olr:
    image: container-registry.oracle.com/database/enterprise:19.3.0.0
    container_name: cdc-oracle-olr
    hostname: cdc-oracle-olr
    shm_size: 4gb
    ports:
      - "1521:1521"
    environment:
      ORACLE_SID: ORCLCDB
      ORACLE_PDB: ORCLPDB1
      ORACLE_PWD: top_secret
      ORACLE_CHARACTERSET: AL32UTF8
      ENABLE_ARCHIVELOG: "true"
    volumes:
      - ./scripts-db:/opt/oracle/scripts/startup
      - oracle_data_cdc:/opt/oracle/oradata
      - oracle_fra_cdc:/opt/oracle/fast_recovery_area
    networks:
      - debezium-net
    healthcheck:
      test: ["CMD", "sqlplus", "-L", "sys/top_secret@//localhost:1521/ORCLCDB as sysdba", "@/dev/null"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'

  openlogreplicator:
    image: bersler/openlogreplicator:debian-13.0-dev
    container_name: openlogreplicator
    hostname: openlogreplicator
    restart: unless-stopped
    user: "1000:1000"
    command: >
      /opt/OpenLogReplicator/OpenLogReplicator
      --config /opt/OpenLogReplicator/scripts/OpenLogReplicator.json
      --log-level info
      --trace 3
    ports:
      - "9000:9000"
    volumes:
      - ./scripts:/opt/OpenLogReplicator/scripts:ro
      - ./olr-checkpoint:/opt/OpenLogReplicator/checkpoint
      - ./olr-log:/opt/OpenLogReplicator/log
      - oracle_data_cdc:/opt/oracle/oradata:ro
      - oracle_fra_cdc:/opt/oracle/fast_recovery_area:ro
    depends_on:
      cdc-oracle-olr:
        condition: service_healthy
    networks:
      - debezium-net
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9000"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

volumes:
  oracle_data_cdc:
  oracle_fra_cdc:
  olr_checkpoint:
  olr_log:

networks:
  debezium-net:
    external: true
    name: debezium-net
```

### 3. Kafka Setup

#### 3.1 Debezium Dockerfile

Create `Dockerfile`:

```dockerfile
FROM quay.io/debezium/connect:3.0

USER root

ENV PLUGIN_DIR=/kafka/connect/plugins
ENV KAFKA_CONNECT_ES_DIR=$PLUGIN_DIR/kafka-connect-elasticsearch
ENV KAFKA_CONNECT_SPOOLDIR_DIR=$PLUGIN_DIR/kafka-connect-spooldir
ENV KAFKA_CONNECT_JDBC_SOURCE_DIR=$PLUGIN_DIR/kafka-connect-jdbc-source

RUN mkdir -p $PLUGIN_DIR

# Avro converter and Schema Registry dependencies
RUN curl -L -o $PLUGIN_DIR/kafka-connect-avro-converter.jar \
    https://packages.confluent.io/maven/io/confluent/kafka-connect-avro-converter/7.3.0/kafka-connect-avro-converter-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/kafka-connect-avro-data.jar \
    https://packages.confluent.io/maven/io/confluent/kafka-connect-avro-data/7.3.0/kafka-connect-avro-data-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/kafka-avro-serializer.jar \
    https://packages.confluent.io/maven/io/confluent/kafka-avro-serializer/7.3.0/kafka-avro-serializer-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/kafka-schema-serializer.jar \
    https://packages.confluent.io/maven/io/confluent/kafka-schema-serializer/7.3.0/kafka-schema-serializer-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/kafka-schema-converter.jar \
    https://packages.confluent.io/maven/io/confluent/kafka-schema-converter/7.3.0/kafka-schema-converter-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/kafka-schema-registry-client.jar \
    https://packages.confluent.io/maven/io/confluent/kafka-schema-registry-client/7.3.0/kafka-schema-registry-client-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/common-config.jar \
    https://packages.confluent.io/maven/io/confluent/common-config/7.3.0/common-config-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/common-utils.jar \
    https://packages.confluent.io/maven/io/confluent/common-utils/7.3.0/common-utils-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/avro.jar \
    https://repo1.maven.org/maven2/org/apache/avro/avro/1.10.2/avro-1.10.2.jar && \
    curl -L -o $PLUGIN_DIR/commons-compress.jar \
    https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar && \
    curl -L -o $PLUGIN_DIR/failureaccess.jar \
    https://repo1.maven.org/maven2/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar && \
    curl -L -o $PLUGIN_DIR/guava.jar \
    https://repo1.maven.org/maven2/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar && \
    curl -L -o $PLUGIN_DIR/minimal-json.jar \
    https://repo1.maven.org/maven2/com/eclipsesource/minimal-json/minimal-json/0.9.5/minimal-json-0.9.5.jar && \
    curl -L -o $PLUGIN_DIR/re2j.jar \
    https://repo1.maven.org/maven2/com/google/re2j/re2j/1.3/re2j-1.3.jar && \
    curl -L -o $PLUGIN_DIR/slf4j-api.jar \
    https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar && \
    curl -L -o $PLUGIN_DIR/snakeyaml.jar \
    https://repo1.maven.org/maven2/org/yaml/snakeyaml/1.30/snakeyaml-1.30.jar && \
    curl -L -o $PLUGIN_DIR/swagger-annotations.jar \
    https://repo1.maven.org/maven2/io/swagger/swagger-annotations/1.6.4/swagger-annotations-1.6.4.jar && \
    curl -L -o $PLUGIN_DIR/jackson-databind.jar \
    https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.13.3/jackson-databind-2.13.3.jar && \
    curl -L -o $PLUGIN_DIR/jackson-core.jar \
    https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.13.3/jackson-core-2.13.3.jar && \
    curl -L -o $PLUGIN_DIR/jackson-annotations.jar \
    https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.13.3/jackson-annotations-2.13.3.jar && \
    curl -L -o $PLUGIN_DIR/jackson-dataformat-csv.jar \
    https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.13.3/jackson-dataformat-csv-2.13.3.jar && \
    curl -L -o $PLUGIN_DIR/logredactor.jar \
    https://repo1.maven.org/maven2/io/confluent/logredactor/7.3.0/logredactor-7.3.0.jar && \
    curl -L -o $PLUGIN_DIR/logredactor-metrics.jar \
    https://repo1.maven.org/maven2/io/confluent/logredactor-metrics/7.3.0/logredactor-metrics-7.3.0.jar

# Additional connectors (copy from local directories)
RUN mkdir $KAFKA_CONNECT_ES_DIR
COPY ./confluentinc-kafka-connect-elasticsearch-14.1.2/lib/ $KAFKA_CONNECT_ES_DIR

RUN mkdir $KAFKA_CONNECT_SPOOLDIR_DIR
COPY ./jcustenborder-kafka-connect-spooldir-2.0.66/lib/ $KAFKA_CONNECT_SPOOLDIR_DIR

RUN mkdir $KAFKA_CONNECT_JDBC_SOURCE_DIR
COPY ./confluentinc-kafka-connect-jdbc-10.8.4/lib/ $KAFKA_CONNECT_JDBC_SOURCE_DIR

# Oracle JDBC drivers with XML support
RUN cd /kafka/libs && \
    curl -sO https://download.oracle.com/otn-pub/otn_software/jdbc/231/ojdbc11.jar && \
    curl -sO https://download.oracle.com/otn-pub/otn_software/jdbc/231/orai18n.jar && \
    curl -sO https://download.oracle.com/otn-pub/otn_software/jdbc/231/xdb.jar && \
    curl -sO https://download.oracle.com/otn-pub/otn_software/jdbc/231/xmlparserv2.jar

# Oracle Instant Client
RUN curl https://download.oracle.com/otn_software/linux/instantclient/2112000/el9/instantclient-basic-linux.x64-21.12.0.0.0dbru.el9.zip -o /tmp/ic.zip && \
    unzip /tmp/ic.zip -d /usr/share/java/debezium-connector-oracle/

# JDBC Sink Connector
RUN mkdir -p /kafka/connect/plugins/jdbc-sink-connector && \
    cd /kafka/connect/plugins/jdbc-sink-connector && \
    curl -L -o jdbc-sink-connector.zip \
    https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-jdbc/versions/10.8.4/confluentinc-kafka-connect-jdbc-10.8.4.zip && \
    unzip jdbc-sink-connector.zip && \
    rm jdbc-sink-connector.zip

# PostgreSQL JDBC driver
RUN cd /kafka/libs && \
    curl -sO https://jdbc.postgresql.org/download/postgresql-42.7.3.jar
```

#### 3.2 Docker Compose - Kafka Ecosystem

Create `docker-compose-main.yml`:

```yaml
services:
  cdc-postgres:
    image: postgres:16
    container_name: cdc-postgres
    ports:
      - 5555:5432
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
    volumes:
      - pg_data_cdc:/var/lib/postgresql/data
    networks:
      - debezium-net

  kafka-1:
    image: apache/kafka:4.1.1
    container_name: kafka-1
    ports:
      - "29093:9093"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT-INTERNAL:PLAINTEXT'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-1:29092,2@kafka-2:29092,3@kafka-3:29092'
      KAFKA_LISTENERS: 'PLAINTEXT-INTERNAL://:19093,CONTROLLER://kafka-1:29092,PLAINTEXT://:9093'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT-INTERNAL'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT-INTERNAL://kafka-1:19093,PLAINTEXT://localhost:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: 'j8T24XD0RMiIMaxLX824wA'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
    volumes:
      - kafka_1_vol:/var/lib/kafka/data
    networks:
      - debezium-net

  kafka-2:
    image: apache/kafka:4.1.1
    container_name: kafka-2
    ports:
      - "39093:9093"
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT-INTERNAL:PLAINTEXT'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-1:29092,2@kafka-2:29092,3@kafka-3:29092'
      KAFKA_LISTENERS: 'PLAINTEXT-INTERNAL://:19093,CONTROLLER://kafka-2:29092,PLAINTEXT://:9093'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT-INTERNAL'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT-INTERNAL://kafka-2:19093,PLAINTEXT://localhost:39093
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: 'j8T24XD0RMiIMaxLX824wA'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
    volumes:
      - kafka_2_vol:/var/lib/kafka/data
    networks:
      - debezium-net

  kafka-3:
    image: apache/kafka:4.1.1
    container_name: kafka-3
    ports:
      - "49093:9093"
    environment:
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT-INTERNAL:PLAINTEXT'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-1:29092,2@kafka-2:29092,3@kafka-3:29092'
      KAFKA_LISTENERS: 'PLAINTEXT-INTERNAL://:19093,CONTROLLER://kafka-3:29092,PLAINTEXT://:9093'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT-INTERNAL'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT-INTERNAL://kafka-3:19093,PLAINTEXT://localhost:49093
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: 'j8T24XD0RMiIMaxLX824wA'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
    volumes:
      - kafka_3_vol:/var/lib/kafka/data
    networks:
      - debezium-net

  schema-registry:
    image: confluentinc/cp-schema-registry:7.8.0
    container_name: schema-registry
    ports:
      - "8081:8081"
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_DEBUG: "true"
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "kafka-1:19093,kafka-2:19093,kafka-3:19093"
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: PLAINTEXT
    volumes:
      - schema_registry_data:/etc/schema-registry/data
    networks:
      - debezium-net

  debezium-kafka-connect:
    build: .
    image: debezium-kafka-connect
    container_name: debezium-kafka-connect
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      GROUP_ID: cdc_connect-group
      BOOTSTRAP_SERVERS: "kafka-1:19093,kafka-2:19093,kafka-3:19093"
      CONFIG_STORAGE_TOPIC: cdc_connect_configs
      OFFSET_STORAGE_TOPIC: cdc_connect_offsets
      STATUS_STORAGE_TOPIC: cdc_connect_status
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_SECURITY_PROTOCOL: "PLAINTEXT"
      CONNECT_PRODUCER_SECURITY_PROTOCOL: "PLAINTEXT"
      CONNECT_CONSUMER_SECURITY_PROTOCOL: "PLAINTEXT"
      CONNECT_PRODUCER_REQUEST_TIMEOUT_MS_CONFIG: 60000
      CONNECT_CONSUMER_REQUEST_TIMEOUT_MS_CONFIG: 60000
    volumes:
      - debezium_plugins:/kafka/connect/plugins
      - debezium_data:/kafka/data
    networks:
      - debezium-net

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "18000:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:19093,kafka-2:19093,kafka-3:19093
      KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: PLAINTEXT
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: debezium-kafka-connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://debezium-kafka-connect:8083
    volumes:
      - kafka-ui-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18000/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry
      - debezium-kafka-connect
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    networks:
      - debezium-net

volumes:
  kafka_1_vol:
  kafka_2_vol:
  kafka_3_vol:
  kafka-ui-data:
  pg_data_cdc:
  oracle_data_cdc:
  debezium_plugins:
  debezium_data:
  schema_registry_data:

networks:
  debezium-net:
    external: true
    name: debezium-net
```

---
## Connector Configuration
### Source Connector
```url: POST http://localhost:8083/connectors```
```json
{
  "name": "oracle-olr-connector-45",
  "config": {
    "connector.class": "io.debezium.connector.oracle.OracleConnector",
    "tasks.max": "1",
    "topic.prefix": "oracle.olr",
    "database.hostname": "cdc-oracle-olr",
    "database.port": "1521",
    "database.user": "c##dbzuser",
    "database.password": "dbz",
    "database.dbname": "ORCLCDB",
    "database.pdb.name": "ORCLPDB1",
    "schema.history.internal.kafka.bootstrap.servers": "kafka-1:19093,kafka-2:19093,kafka-3:19093",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter.schema.registry.url": "http://schema-registry:8081",
    "schema.history.internal.kafka.topic": "schema-changes.oracle",
    "database.connection.adapter": "olr",
    "openlogreplicator.host": "openlogreplicator",
    "openlogreplicator.port": "9000",
    "openlogreplicator.source": "ORACLE",   
    "snapshot.mode": "initial",
    "decimal.handling.mode": "string",
    "time.precision.mode": "adaptive"
  }
}
```
### Sink Connector
```url: PUT http://localhost:8083/connectors/sink-olr-103/config```
```json
{
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "connection.url": "jdbc:postgresql://cdc-postgres:5432/",
    "connection.user": "postgres",
    "connection.password": "postgres",
    "topics": "oracle.olr.OLR_DB.PRODUCT",
    "tasks.max": "1",
    "auto.create": "true",
    "auto.evolve": "true",
    "insert.mode": "upsert",
    "pk.mode": "record_key",
    "pk.fields": "ID",
    "table.name.format": "products",
    "transforms": "unwrap",
    "delete.enabled": "true",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": "false",
    "transforms.unwrap.delete.handling.mode": "rewrite"
}
```

---

## Verification

After the CDC connector is running, test the pipeline with sample data:

```bash
# Connect to Oracle
sqlplus olr_db/olr_password@ORCLPDB1

# Insert test data
INSERT INTO product (name, description, price, stock)
VALUES ('Test Product', 'CDC Test', 99.99, 10);
COMMIT;

# Update test data
UPDATE product SET price = 149.99 WHERE id = 1;
COMMIT;

# Delete test data
DELETE FROM product WHERE id = 1;
COMMIT;
```

Check Kafka UI at `http://localhost:18000` to verify messages are flowing.

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| Oracle container fails to start | Ensure sufficient memory (8GB minimum) and disk space |
| OpenLogReplicator cannot connect | Verify Oracle is healthy and permissions are set correctly |
| No messages in Kafka | Check connector status via Kafka Connect REST API |
| Schema Registry errors | Ensure Kafka brokers are running before Schema Registry starts |

### Useful Commands

```bash
# Check container logs
docker logs --tail 50 --follow cdc-oracle-olr
docker logs --tail 50 --follow openlogreplicator
docker logs --tail 50 --follow debezium-kafka-connect

# Check Kafka Connect status
curl http://localhost:8083/connectors

# Check connector status
curl http://localhost:8083/connectors/<connector-name>/status
```
